{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "@Title   :  Âü∫‰∫éChatGLM2-6B+LoRAÂú®ÊôÆÊÉ†ÈáëËûçÊîøÁ≠ñÊï∞ÊçÆÈõÜ‰∏äËøõË°åÂæÆË∞É\n",
    "@Time    :   2023/7/30 \n",
    "@Author  :   Áéã‰Ω≥Êô®\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /root/miniconda3/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain libcudart.so as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('Asia/Shanghai')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('//10.0.2.207'), PosixPath('17606')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64'), PosixPath('https')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel, TrainingArguments, AutoConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/autodl-tmp/ChatGLM-Tuning'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ÂØπÊñáÊú¨ËøõË°åtokenize,Âä†Âø´ËÆ≠ÁªÉÈÄüÂ∫¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached dataset generator (/root/.cache/huggingface/datasets/generator/default-75c4361790b22033/0.0.0)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "!python tokenize_dataset_rows.py \\\n",
    "    --jsonl_path data/puhui227_prepared.jsonl \\\n",
    "    --save_path data/puhui \\\n",
    "    --chatglm_path THUDM/chatglm2-6b \\\n",
    "    --version v1                 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ê®°ÂûãËÆ≠ÁªÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2956efa75abb4cffa57556ab580bf5e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08218d49d61e4364995ad58941bab6d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)l-00001-of-00007.bin:   0%|          | 0.00/1.83G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4fc98ac3b314e9e9cf15fa6a272db57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)l-00002-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7508dff7fc343abb69f8f87501b8f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)l-00003-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d554980bd8842a18c2abfcb5e0ba1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)l-00004-of-00007.bin:   0%|          | 0.00/1.82G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652f1f3c184740b8b7f5cee1cbb67b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)l-00005-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39378c73b2a74fb4b848242a6f64ac82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)l-00006-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f82f62ffd2a4ae589d8c28fe97635e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)l-00007-of-00007.bin:   0%|          | 0.00/1.05G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdb55bc49f364fd9870482a6ad168e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): return super().forward(x).to(torch.float32)\n",
    "\n",
    "\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", load_in_8bit=True, trust_remote_code=True, device_map='auto')\n",
    "model.supports_gradient_checkpointing = True\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "model.config.use_cache = False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32, lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.is_parallelizable = True\n",
    "model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "dataset_path = \"data/puhui\"\n",
    "\n",
    "dataset = datasets.load_from_disk(dataset_path)\n",
    "\n",
    "train_num = 500\n",
    "\n",
    "mini_train_dataset = datasets.Dataset.from_dict(dataset[:train_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, HfArgumentParser\n",
    "\n",
    "\n",
    "def data_collator(features: list) -> dict:\n",
    "    len_ids = [len(feature[\"input_ids\"]) for feature in features]\n",
    "    longest = max(len_ids)\n",
    "    input_ids = []\n",
    "    labels_list = []\n",
    "    for ids_l, feature in sorted(zip(len_ids, features), key=lambda x: -x[0]):\n",
    "        ids = feature[\"input_ids\"]\n",
    "        seq_len = feature[\"seq_len\"]\n",
    "        labels = (\n",
    "            [-100] * (seq_len - 1) + ids[(seq_len - 1) :] + [-100] * (longest - ids_l)\n",
    "        )\n",
    "        ids = ids + [tokenizer.pad_token_id] * (longest - ids_l)\n",
    "        _ids = torch.LongTensor(ids)\n",
    "        labels_list.append(torch.LongTensor(labels))\n",
    "        input_ids.append(_ids)\n",
    "    input_ids = torch.stack(input_ids)\n",
    "    labels = torch.stack(labels_list)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "class ModifiedTrainer(Trainer):\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        return model(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            labels=inputs[\"labels\"],\n",
    "        ).loss\n",
    "    \n",
    "def save_tunable_parameters(model, path):\n",
    "    saved_params = {\n",
    "        k: v.to(\"cpu\") for k, v in model.named_parameters() if v.requires_grad\n",
    "    }\n",
    "    torch.save(saved_params, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 05:12, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.547200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.615700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.691400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.547200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.441700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.111300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.132900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.105400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.155700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.667100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.668700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.634800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:391: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=600, training_loss=2.193267822265625, metrics={'train_runtime': 313.0753, 'train_samples_per_second': 1.916, 'train_steps_per_second': 1.916, 'total_flos': 5044623235448832.0, 'train_loss': 2.193267822265625, 'epoch': 2.64})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"output\",\n",
    "    fp16 =False,\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    learning_rate = 1e-3,\n",
    "    max_steps=600,\n",
    "    logging_steps=50,\n",
    "    remove_unused_columns=False,\n",
    "    seed=0,\n",
    "    data_seed=0,\n",
    "    group_by_length=False,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = ModifiedTrainer(\n",
    "    model=model,\n",
    "    train_dataset=mini_train_dataset,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_tunable_parameters(\n",
    "        model, os.path.join(\"output\", \"chatglm2-lora.pt\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26cf6cf895e3403ba6dad847da6c6ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/__init__.py:615: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True).half().cuda()\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "peft_path = \"output/chatglm2-lora.pt\"\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.load_state_dict(torch.load(peft_path), strict=False)\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ÂæÆË∞ÉÊïàÊûú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‰Ω†Â•ΩüëãÔºÅÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM2-6BÔºåÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†ÔºåÊ¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"‰Ω†Â•Ω\", history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÊôÆÊÉ†ÈáëËûçÂèëÂ±ïÊîøÁ≠ñÂ¶Ç‰∏ã:\n",
      "\n",
      "1. „ÄäÊé®ËøõÊôÆÊÉ†ÈáëËûçÂèëÂ±ïËßÑÂàí(2016-2020Âπ¥)„Äã(ÂõΩÂäûÂèë[2015]74Âè∑)\n",
      "\n",
      "2. „ÄäÂÖ≥‰∫éÊîØÊåÅÈáëËûçÊú∫ÊûÑËøõ‰∏ÄÊ≠•ÊîπËøõÂ∞èÂûãÂæÆÂûã‰ºÅ‰∏öÈáëËûçÊúçÂä°(‰øÆËÆ¢)„Äã\n",
      "\n",
      "3. „ÄäÂÖ≥‰∫éÁßØÊûÅÂèëÂ±ïÊôÆÊÉ†ÈáëËûçÁöÑÊåáÂØºÊÑèËßÅ„Äã(Èì∂Âèë[2015]25Âè∑)\n",
      "\n",
      "4. „ÄäÂÖ≥‰∫éÊôÆÊÉ†ÈáëËûçÁªüËÆ°Â∑•‰ΩúÁöÑÊåáÂØºÊÑèËßÅ„Äã(Èì∂Âèë[2015]26Âè∑)\n",
      "\n",
      "5. „ÄäÊôÆÊÉ†ÈáëËûçÂèëÂ±ïËßÑÂàí(2016-2020Âπ¥)„Äã(‰∏≠ÂõΩ‰∫∫Ê∞ëÈì∂Ë°å„ÄÅ‰∏≠ÂõΩÈì∂Ë°å‰øùÈô©ÁõëÁù£ÁÆ°ÁêÜÂßîÂëò‰ºö„ÄÅÂõΩÂÆ∂ÂèëÂ±ïÊîπÈù©Âßî„ÄÅË¥¢ÊîøÈÉ®„ÄÅÂ∑•‰∏öÂíå‰ø°ÊÅØÂåñÈÉ®„ÄÅË¥¢ÊîøÈÉ®„ÄÅËØÅÁõë‰ºö„ÄÅ‰øùÁõë‰ºö„ÄÅÂõΩÂÆ∂Â§ñÊ±áÁÆ°ÁêÜÂ±Ä2015Âπ¥11Êúà28Êó•ÂÖ¨Â∏É)\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"Âàó‰∏æ‰∏Ä‰∫õÊôÆÊÉ†ÈáëËûçÂèëÂ±ïÊîøÁ≠ñ\", history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "„ÄäÂÖ≥‰∫éÁßØÊûÅÂèëÂ±ïÊôÆÊÉ†ÈáëËûçÁöÑÊåáÂØºÊÑèËßÅ„ÄãÊòØ2015Âπ¥4Êúà29Êó•‰∏≠ÂõΩ‰∫∫Ê∞ëÈì∂Ë°åÁ≠âÂçÅÈÉ®ÂßîËÅîÂêàÂèëÂ∏ÉÁöÑ‰∏Ä‰ªΩÊñá‰ª∂ÔºåÊó®Âú®Êé®Âä®ÊôÆÊÉ†ÈáëËûç‰∏öÂä°ÂÅ•Â∫∑ÂèëÂ±ïÔºåÂä©ÂäõÁªèÊµéÂ¢ûÈïø„ÄÇÊôÆÊÉ†ÈáëËûçÊòØÊåáÈáëËûçÊú∫ÊûÑÈÄöËøáÂàõÊñ∞ÈáëËûç‰∫ßÂìÅÂíåÊúçÂä°ÊñπÂºèÔºåÂ∞ÜÈáëËûçÊúçÂä°Êõ¥Âä†ÊôÆÂèäÂíå‰æøÊç∑Ôºå‰ª•Êª°Ë∂≥Â∞èÂæÆ‰ºÅ‰∏ö„ÄÅÂÜúÊ∞ë„ÄÅÂüéÈïá‰ΩéÊî∂ÂÖ•‰∫∫Áæ§„ÄÅË¥´Âõ∞‰∫∫Áæ§Á≠âÂº±ÂäøÁæ§‰ΩìÁöÑÈáëËûçÊúçÂä°ÈúÄÊ±Ç„ÄÇ\n",
      "\n",
      "Âú®Êé®Âä®ÊôÆÊÉ†ÈáëËûçÂèëÂ±ïËøáÁ®ã‰∏≠ÔºåÈúÄË¶ÅÊ≥®ÊÑè‰ª•‰∏ãÂá†ÁÇπÔºö\n",
      "\n",
      "1. Âº∫ÂåñÈáëËûçÂü∫Á°ÄËÆæÊñΩÂª∫ËÆæÔºåÊèêÂçáÈáëËûçÊúçÂä°ÁöÑÊôÆÂèäÁéá„ÄÇÂèëÂ±ïÊôÆÊÉ†ÈáëËûçÈúÄË¶ÅÂª∫Á´ãËâØÂ•ΩÁöÑÈáëËûçÁîüÊÄÅÁéØÂ¢ÉÔºåËøôÈúÄË¶ÅÂª∫Á´ãÂÆåÂñÑÁöÑÂü∫Á°ÄËÆæÊñΩÔºåÂ¶ÇÁΩëÁÇπ„ÄÅÁΩëÁªú„ÄÅÊîØ‰ªòÁ≥ªÁªüÁ≠âÔºåÂπ∂‰∏çÊñ≠ÊèêÂçáÊúçÂä°Ê∞¥Âπ≥ÔºåÁ°Æ‰øùÈáëËûçÊúçÂä°ÁöÑÊôÆÂèäÁéáÈÄêÂπ¥ÊèêÈ´ò„ÄÇ\n",
      "\n",
      "2. ‰∏∞ÂØåÊôÆÊÉ†ÈáëËûç‰∫ßÂìÅÂíåÊúçÂä°ÔºåÊª°Ë∂≥‰∏çÂêå‰∫∫Áæ§ÈúÄÊ±Ç„ÄÇÂèëÂ±ïÊôÆÊÉ†ÈáëËûçÈúÄË¶ÅÈíàÂØπ‰∏çÂêå‰∫∫Áæ§ÁöÑÈúÄÊ±ÇÔºåÊèê‰æõÂ§öÊ†∑ÂåñÁöÑÈáëËûç‰∫ßÂìÅÂíåÊúçÂä°ÔºåÂ¶ÇË¥∑Ê¨æ„ÄÅ‰øùÈô©„ÄÅÁêÜË¥¢Á≠âÔºå‰ª•Êª°Ë∂≥Â∞èÂæÆ‰ºÅ‰∏ö„ÄÅÂÜúÊ∞ë„ÄÅÂüéÈïá‰ΩéÊî∂ÂÖ•‰∫∫Áæ§„ÄÅË¥´Âõ∞‰∫∫Áæ§Á≠âÂº±ÂäøÁæ§‰ΩìÁöÑÈúÄÊ±Ç„ÄÇ\n",
      "\n",
      "3. ÂàõÊñ∞ÈáëËûçÊ®°ÂºèÔºå‰∏∞ÂØåÈáëËûç‰æõÁªô„ÄÇÂèëÂ±ïÊôÆÊÉ†ÈáëËûçÈúÄË¶Å‰∏çÊñ≠Êé¢Á¥¢Êñ∞ÁöÑÈáëËûçÊ®°ÂºèÔºåÂ¶ÇÁΩëÁªúÈì∂Ë°å„ÄÅÁßªÂä®ÊîØ‰ªò„ÄÅË¥∑Ê¨æË¥¥Áé∞Á≠âÔºå‰∏∞ÂØåÈáëËûç‰æõÁªôÔºåÊèêÈ´òÊúçÂä°ÊïàÁéá„ÄÇ\n",
      "\n",
      "4. Âº∫ÂåñÁõëÁÆ°ÂíåÂºïÂØºÔºåËßÑËåÉÂ∏ÇÂú∫Áß©Â∫è„ÄÇÂèëÂ±ïÊôÆÊÉ†ÈáëËûçÈúÄË¶ÅÂº∫ÂåñÁõëÁÆ°ÂíåÂºïÂØºÔºåËßÑËåÉÂ∏ÇÂú∫Áß©Â∫èÔºåÊâìÂáªÈùûÊ≥ïÈáëËûçÊ¥ªÂä®Ôºå‰øùÈöúÈáëËûçÊ∂àË¥πËÄÖÁöÑÂêàÊ≥ïÊùÉÁõäÔºåÁª¥Êä§ÈáëËûçÂ∏ÇÂú∫Áß©Â∫è„ÄÇ\n",
      "\n",
      "5. Âä†Âº∫ÊîøÁ≠ñÊîØÊåÅÔºåËê•ÈÄ†ËâØÂ•ΩÁéØÂ¢É„ÄÇÂèëÂ±ïÊôÆÊÉ†ÈáëËûçÈúÄË¶ÅÊîøÁ≠ñÊîØÊåÅÔºåËê•ÈÄ†ËâØÂ•ΩÁöÑÂ∏ÇÂú∫ÁéØÂ¢É„ÄÇÊîøÂ∫úÂ∫îÂΩìÂä†Â§ßÂØπÊôÆÊÉ†ÈáëËûçÁöÑÊîøÁ≠ñÊîØÊåÅÂäõÂ∫¶ÔºåÊé®Âä®ÈáëËûçÊú∫ÊûÑÂä†Â§ßÂØπÊôÆÊÉ†ÈáëËûçÁöÑÊäïÂÖ•ÔºåÂºïÂØºÈáëËûçÂ∏ÇÂú∫ÂÅ•Â∫∑ÂèëÂ±ï„ÄÇ\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"ÈÇ£ËØ∑‰Ω†Ê¶ÇÊã¨‰∏Ä‰∏ã„ÄäÂÖ≥‰∫éÁßØÊûÅÂèëÂ±ïÊôÆÊÉ†ÈáëËûçÁöÑÊåáÂØºÊÑèËßÅ„ÄãÂπ∂ËØ¥Âá∫Êñá‰ª∂Êúâ‰ªÄ‰πàÈúÄË¶ÅÁâπÂà´Ê≥®ÊÑèÁöÑÂú∞Êñπ\", history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "„ÄäÊôÆÊÉ†ÈáëËûçÂèëÂ±ïËßÑÂàíÔºà2016-2020Âπ¥Ôºâ„ÄãÊòØ‰∏∫Ê∑±ÂÖ•Ë¥ØÂΩªËêΩÂÆûÂÖö‰∏≠Â§Æ„ÄÅÂõΩÂä°Èô¢ÂÖ≥‰∫éÈáëËûçÊîØÊåÅÂÆû‰ΩìÁªèÊµéÂèëÂ±ïÁöÑÂÜ≥Á≠ñÈÉ®ÁΩ≤ÔºåËêΩÂÆûÂÖöÁöÑÂçÅÂÖ´Â§ßÂíåÂçÅÂÖ´Â±ä‰∏â‰∏≠„ÄÅÂõõ‰∏≠„ÄÅ‰∫î‰∏≠ÂÖ®‰ºöÁ≤æÁ•ûÔºåÊåâÁÖß‚ÄúÂçÅ‰∏â‰∫î‚ÄùÊó∂ÊúüÊé®Ëøõ‰æõÁªô‰æßÁªìÊûÑÊÄßÊîπÈù©Ë¶ÅÊ±ÇÔºåÊåâÁÖß‚ÄúÊôÆÊÉ†ÊÄß„ÄÅÂèØËé∑ÂæóÊÄß„ÄÅÁÅµÊ¥ªÊÄß„ÄÅÂèØÊåÅÁª≠ÊÄß‚ÄùÂõõÂ§ßÊ†∏ÂøÉË¶ÅÁ¥†ÔºåÂùöÊåÅ‚ÄúÊîøÂ∫úÂºïÂØº„ÄÅÂ∏ÇÂú∫‰∏ªÂØº‚ÄùÁöÑÊôÆÊÉ†ÈáëËûçÂèëÂ±ïÁêÜÂøµÔºåÂÅ•ÂÖ®ÊøÄÂä±Êú∫Âà∂ÔºåÊãìÂÆΩÂèëÂ±ïÁ©∫Èó¥ÔºåÂÆåÂñÑÊôÆÊÉ†ÈáëËûçÊúçÂä°‰ΩìÁ≥ªÔºåÂÖ®Èù¢ÊèêÈ´òÈáëËûçÊúçÂä°ÂÆû‰ΩìÁªèÊµéËÉΩÂäõÔºåÂÆûÁé∞ÊôÆÊÉ†ÈáëËûçÂèØÊåÅÁª≠ÂèëÂ±ïÔºå‰∏∫ÁªèÊµéÁ§æ‰ºöÂèëÂ±ïÊèê‰æõÊñ∞Âä®ËÉΩ„ÄÅÊñ∞ÊîØÊíëÔºåÂÆûÁé∞‚Äú‰∏âÂÜú‚ÄùÂíåÊîØÂ∞èÊîØÂæÆ„ÄÅÁ≤æÂáÜÊâ∂Ë¥´Á≤æÂáÜËÑ±Ë¥´„ÄÅÂ∞èÂæÆ‰ºÅ‰∏öÈáëËûçÊúçÂä°Ë¶ÜÁõñÁéáÊòéÊòæÊèêÈ´òÔºå‰ΩøÊØè‰∏™Â∏ÇÂú∫‰∏ª‰ΩìÂú®ÈáëËûçÊúçÂä°‰∏≠ÂèóÁõäÔºå‰ΩøÊØè‰∏™ÊôÆÈÄöÊ∂àË¥πËÄÖÈÉΩÊúâÊõ¥Â•ΩÁöÑÈáëËûçËé∑ÂæóÊÑüÔºå‰ΩøÈáëËûçÊõ¥Â•ΩÂú∞ÊúçÂä°ÂÆû‰ΩìÁªèÊµéÔºåÂú®ÂÖ®Èù¢Âª∫ÊàêÂ∞èÂ∫∑Á§æ‰ºöÂü∫Á°Ä‰∏äÔºåÂà∞2020Âπ¥ÔºåÊôÆÊÉ†ÈáëËûçÊúçÂä°Ë¶ÜÁõñÁéá‰øùÊåÅÂú®ËæÉÈ´òÊ∞¥Âπ≥ÔºåÊôÆÊÉ†ÈáëËûçÊúçÂä°ÈáçÁÇπÈ¢ÜÂüüË¶ÜÁõñÁéáÊòéÊòæÊèêÈ´òÔºåÊØè‰∏™ÂéøËá≥Â∞ëÊúâ1ÂÆ∂‰ª•‰∏äÂÖ∑Â§áÊù°‰ª∂ÁöÑÈáëËûçÊú∫ÊûÑÊèê‰æõÂü∫Á°ÄÈáëËûçÊúçÂä°ÔºåÊØè‰∏™ÂéøËá≥Â∞ëÊúâ1ÂÆ∂‰ª•‰∏äÂÖ∑Â§áÊù°‰ª∂ÁöÑÈáëËûçÊú∫ÊûÑÊèê‰æõÂ∞èÂæÆ‰ºÅ‰∏öÈáëËûçÊúçÂä°„ÄÇ\n",
      "\n",
      "‰∏Ä„ÄÅÂèëÂ±ïÂü∫Á°Ä\n",
      "\n",
      "Ôºà‰∏ÄÔºâÂèëÂ±ïÁé∞Áä∂„ÄÇÂΩìÂâçÔºåÊàëÂõΩÈáëËûç‰ΩìÁ≥ªÂü∫Êú¨ÂÅ•ÂÖ®Ôºå‰ΩÜÈáëËûçËµÑÊ∫êÁõ∏ÂØπ‰∏çË∂≥ÔºåÁªìÊûÑ‰∏çÂùáË°°ÔºåÂèëÂ±ïÂèØÊåÅÁª≠ÊÄßÊúâÂæÖÊèêÈ´ò„ÄÇÂü∫Á°ÄÈáëËûçÊúçÂä°ÊôÆÂèäÁéá‰∏çÊñ≠ÊèêÈ´òÔºå‰ΩÜÈÉ®ÂàÜÂå∫Âüü„ÄÅÈ¢ÜÂüüÂíå‰∫∫Áæ§ÁöÑÈáëËûçÊúçÂä°ÈúÄÊ±Ç‰ªçÁÑ∂Èöæ‰ª•Êª°Ë∂≥„ÄÇÂ∞èÂæÆ‰ºÅ‰∏ö„ÄÅÂÜúÊ∞ë„ÄÅÊÆãÁñæ‰∫∫„ÄÅËÄÅÂπ¥‰∫∫Á≠âÁâπÊÆäÁæ§‰ΩìÂíåÂå∫ÂüüÊÄßË¥´Âõ∞Âú∞Âå∫„ÄÅÊ¨†ÂèëËææÂú∞Âå∫‰ªçÁÑ∂Â≠òÂú®ÈáëËûçÊúçÂä°ÈöæÁÇπÁöÑÂà∂Á∫¶„ÄÇ\n",
      "\n",
      "Ôºà‰∫åÔºâÂèëÂ±ïÂâçÊôØ„ÄÇÈöèÁùÄÁªèÊµéÂèëÂ±ïÂíå‰∫∫Ê∞ëÁæ§‰ºóÁîüÊ¥ªÊîπÂñÑÔºå‰∫∫Ê∞ëÁæ§‰ºóÂØπÈáëËûçÊúçÂä°ÁöÑÈúÄÊ±ÇÊó•ÁõäÂ¢ûÈïøÔºåÂ§öÂ±ÇÊ¨°„ÄÅÂπøË¶ÜÁõñ„ÄÅ‰æøÊç∑ÂåñÁöÑÈáëËûçÊúçÂä°Â∞ÜÂæóÂà∞Êõ¥‰∏∫ÂπøÈòîÁöÑÂ∏ÇÂú∫ÈúÄÊ±ÇÔºåÊú™Êù•‰∏ÄÊÆµÊó∂ÊúüÊàëÂõΩÊôÆÊÉ†ÈáëËûçÂèëÂ±ïÂâçÊôØÂπøÈòî„ÄÇÂêåÊó∂ÔºåÊñ∞‰∏Ä‰ª£‰ø°ÊÅØÊäÄÊúØ„ÄÅÂ§ßÊï∞ÊçÆ„ÄÅ‰∫ëËÆ°ÁÆó„ÄÅÂå∫ÂùóÈìæÁ≠âÈáëËûçÁßëÊäÄÁöÑÂèëÂ±ïÔºåÂ∞ÜÊé®Âä®ÈáëËûçÊúçÂä°ÁöÑÂàõÊñ∞‰∏éËΩ¨ÂûãÔºå‰∏∫ÊôÆÊÉ†ÈáëËûçÂèëÂ±ïÊèê‰æõÊñ∞Êú∫ÈÅá„ÄÇ\n",
      "\n",
      "‰∫å„ÄÅÂèëÂ±ïÁõÆÊ†á\n",
      "\n",
      "Ôºà‰∏ÄÔºâÊÄª‰ΩìÁõÆÊ†á„ÄÇÊú™Êù•‰∫îÂπ¥ÔºåÂü∫Êú¨ÂÆûÁé∞ÂÖ®ÂõΩÊØè‰∏™ÂéøËá≥Â∞ëÊúâ1ÂÆ∂‰ª•‰∏äÂÖ∑Â§áÊù°‰ª∂ÁöÑÈáëËûçÊú∫ÊûÑÊèê‰æõÂü∫Á°ÄÈáëËûçÊúçÂä°ÔºåÊØè‰∏™ÂéøËá≥Â∞ëÊúâ1ÂÆ∂‰ª•‰∏äÂÖ∑Â§áÊù°‰ª∂ÁöÑÈáëËûçÊú∫ÊûÑÊèê‰æõÂ∞èÂæÆ‰ºÅ‰∏öÈáëËûçÊúçÂä°„ÄÇÂ∞èÂæÆ‰ºÅ‰∏öË¥∑Ê¨æÂπ≥ÂùáÂà©ÁéáÊòéÊòæ‰∏ãÈôçÔºåÂ∞èÂæÆ‰ºÅ‰∏öË¥∑Ê¨æ‰∏çËâØÁéáÊòéÊòæ‰∏ãÈôç„ÄÇÂü∫Á°ÄÈáëËûçÊúçÂä°Ë¶ÜÁõñÁéá‰øùÊåÅÂú®90%‰ª•‰∏äÔºåÊØè‰∏™ÂéøËá≥Â∞ëÊúâ1ÂÆ∂‰ª•‰∏äÂÖ∑Â§áÊù°‰ª∂ÁöÑÈáëËûçÊú∫ÊûÑÊèê‰æõÂü∫Á°ÄÈáëËûçÊúçÂä°ÔºåÊØè‰∏™ÂéøËá≥Â∞ëÊúâ1ÂÆ∂‰ª•‰∏äÂÖ∑Â§áÊù°‰ª∂ÁöÑÈáëËûçÊú∫ÊûÑÊèê‰æõÂ∞èÂæÆ‰ºÅ‰∏öÈáëËûçÊúçÂä°„ÄÇÊôÆÊÉ†ÈáëËûçÈáçÁÇπÈ¢ÜÂüüË¶ÜÁõñÁéáÊòéÊòæÊèêÈ´òÔºåÊØè‰∏™ÂéøËá≥Â∞ëÊúâ1ÂÆ∂‰ª•‰∏äÂÖ∑Â§áÊù°‰ª∂ÁöÑÈáëËûçÊú∫ÊûÑÊèê‰æõÂä©ÂÜúÈáëËûçÊúçÂä°„ÄÇÂü∫Êú¨ÂÆûÁé∞Êñ∞Á∫≥ÂÖ•Êâ∂Ë¥´ÊîøÁ≠ñËåÉÂõ¥ÁöÑÊúâÂä≥Âä®ËÉΩÂäõ‰∏îÊúâÁ®≥ÂÆöÊî∂ÂÖ•Êù•Ê∫êÁöÑË¥´Âõ∞ÂÆ∂Â∫≠Ëá≥Â∞ëÊã•Êúâ1ÁßçÊñπ‰æøÂø´Êç∑ÁöÑÊâ∂Ë¥´Êâ∂Ë¥´ÊñπÂºè„ÄÇ\n",
      "\n",
      "Ôºà‰∫åÔºâÂàÜÁ±ªÊé®Ëøõ„ÄÇ‰∏çÂêåÂú∞Âå∫„ÄÅ‰∏çÂêåÁ±ªÂûãÊú∫ÊûÑÂØπÈáëËûçÊúçÂä°ÁöÑ‰æõÁªôË¶ÅÊ±Ç‰∏çÂêå„ÄÇÈáëËûçËµÑÊ∫êÁõ∏ÂØπÂÖÖË∂≥ÁöÑÂú∞Âå∫ÔºåË¶ÅÂä†Âø´ÂèëÂ±ïÔºåÊèêÈ´òÊúçÂä°ÊïàÁéáÔºõÈáëËûçËµÑÊ∫êÁõ∏ÂØπ‰∏çË∂≥ÁöÑÂú∞Âå∫ÔºåË¶ÅÂä†Â§ßÊäïÂÖ•ÔºåÂä†Âø´ÂèëÂ±ïÈÄüÂ∫¶ÔºõÈáëËûçËµÑÊ∫êÁõ∏ÂØπËñÑÂº±ÁöÑÂú∞Âå∫ÔºåË¶ÅÁ™ÅÂá∫ÈáçÁÇπÔºåÂä†Â§ßÊâ∂ÊåÅÂäõÂ∫¶„ÄÇ\n",
      "\n",
      "Ôºà‰∏âÔºâÂ∑ÆÂºÇÂåñÊé®Ëøõ„ÄÇÂ∞èÂæÆ‰ºÅ‰∏ö„ÄÅÂÜúÊ∞ë„ÄÅÊÆãÁñæ‰∫∫„ÄÅËÄÅÂπ¥‰∫∫Á≠âÁâπÊÆäÁæ§‰ΩìÂíåÂå∫ÂüüÊÄßË¥´Âõ∞Âú∞Âå∫„ÄÅÊ¨†ÂèëËææÂú∞Âå∫ÔºåÂØπÈáëËûçÊúçÂä°ÁöÑ‰æõÁªôË¶ÅÊ±ÇÊõ¥È´òÔºåË¶ÅÂä†Â§ßÊâ∂ÊåÅÂäõÂ∫¶„ÄÇÂ§ßÂûãÈì∂Ë°å„ÄÅËÇ°‰ªΩÂà∂Èì∂Ë°å„ÄÅÈÇÆÊîøÂÇ®ËìÑÈì∂Ë°åË¶ÅÂèëÊå•ÁΩëÁÇπ‰ºòÂäøÔºåÂä†Âø´ÂèëÂ±ïÊôÆÊÉ†ÈáëËûç„ÄÇËÇ°‰ªΩÂà∂Èì∂Ë°å„ÄÅÂú∞ÊñπÊÄßÊ≥ï‰∫∫Èì∂Ë°å„ÄÅÊ∞ëËê•Èì∂Ë°å„ÄÅÂÜúÊùë‰∏≠Â∞èÈì∂Ë°åÊú∫ÊûÑË¶ÅÈÄÇÂ∫îÂ∞èÂæÆ‰ºÅ‰∏öÈáëËûçÊúçÂä°ÈúÄÊ±ÇÔºåÂä†Âø´ÂèëÂ±ï„ÄÇÈáëËûçÁßüËµÅÂÖ¨Âè∏„ÄÅÊ±ΩËΩ¶ÈáëËûçÂÖ¨Âè∏„ÄÅÊ∂àË¥πÈáëËûçÂÖ¨Âè∏Á≠âÊñ∞ÂûãÈáëËûçÊú∫ÊûÑË¶ÅÈÄÇÂ∫îÂ∞èÂæÆ‰ºÅ‰∏öÈáëËûçÊúçÂä°ÈúÄÊ±ÇÔºåÂä†Âø´ÂèëÂ±ï„ÄÇ\n",
      "\n",
      "‰∏â„ÄÅÈáçÁÇπ‰ªªÂä°\n",
      "\n",
      "Ôºà‰∏ÄÔºâÊèêÈ´òÂü∫Á°ÄÈáëËûçÊúçÂä°Ë¶ÜÁõñÁéá„ÄÇË¶ÅÂ§ßÂπÖÊãìÂ±ïÂü∫Á°ÄÈáëËûçÊúçÂä°Ë¶ÜÁõñËåÉÂõ¥ÔºåÊèêÈ´òÊúçÂä°ÊïàÁéáÔºåÁ°Æ‰øùÊØè‰∏™ÂéøËá≥Â∞ëÊúâ1ÂÆ∂‰ª•‰∏äÂÖ∑Â§áÊù°‰ª∂ÁöÑÈáëËûçÊú∫ÊûÑÊèê‰æõÂü∫Á°ÄÈáëËûçÊúçÂä°ÔºåÊØè‰∏™ÂéøËá≥Â∞ëÊúâ1ÂÆ∂‰ª•‰∏äÂÖ∑Â§áÊù°‰ª∂ÁöÑÈáëËûçÊú∫ÊûÑÊèê‰æõÂ∞èÂæÆ‰ºÅ‰∏öÈáëËûçÊúçÂä°„ÄÇË¶ÅÂä†Âø´ÂÆåÂñÑ‰π°Èïá„ÄÅË°óÈÅì„ÄÅÁ§æÂå∫Á≠âÂü∫Â±ÇÊú∫ÊûÑÂ∏ÉÂ±ÄÔºåÊîØÊåÅÂú®Á§æÂå∫„ÄÅ‰π°Èïá„ÄÅÊùëÁ≠âËÆæÁ´ãÂ∞èÂûãÁªèËê•Êú∫ÊûÑÔºå‰∏∫Â∞èÂæÆ‰ºÅ‰∏ö„ÄÅÊùëÔºàÂ±ÖÔºâÊ∞ë„ÄÅËÄÅÂπ¥‰∫∫Êèê‰æõ‰æøÊç∑ÁöÑÈáëËûçÊúçÂä°„ÄÇ\n",
      "\n",
      "Ôºà‰∫åÔºâÊèêÈ´òÂ∞èÂæÆ‰ºÅ‰∏öÈáëËûçÊúçÂä°ÈáçÁÇπÈ¢ÜÂüüË¶ÜÁõñÁéá„ÄÇË¶ÅÈáçÁÇπÊîØÊåÅÂ∞èÂæÆ‰ºÅ‰∏ö„ÄÅ‚Äú‰∏âÂÜú‚ÄùÂíåÊîØÂ∞èÊîØÂæÆ„ÄÅÁ≤æÂáÜÊâ∂Ë¥´Á≤æÂáÜËÑ±Ë¥´„ÄÅÂ∞èÂæÆ‰ºÅ‰∏öÊúçÂä°ÈáçÁÇπÈ¢ÜÂüü„ÄÇÊîØÊåÅÂ∞èÂæÆ‰ºÅ‰∏ö‰ª•Âàõ‰∏öÂàõÊñ∞„ÄÅÁßëÊäÄËµãËÉΩ„ÄÅ‰∫ß‰∏öËûçÂêà„ÄÅ‰∫ßÂìÅÂàõÊñ∞„ÄÅÁÆ°ÁêÜÂàõÊñ∞„ÄÅ‰∫∫ÊâçÂàõÊñ∞Á≠â‰∏∫ÊñπÂêëÔºåÂèëÂ±ïÂàõÊÑèÂûã„ÄÅÁßëÊäÄÂûã„ÄÅ‰∏ì‰∏öÂûã„ÄÅÂàõÊñ∞ÂûãÂ∞èÂæÆ‰ºÅ‰∏ö„ÄÇÊîØÊåÅÈì∂Ë°å‰∏öÈáëËûçÊú∫ÊûÑÂú®ÊúçÂä°Ë¥´Âõ∞Âú∞Âå∫„ÄÅË¥´Âõ∞‰∫∫Âè£ËøáÁ®ã‰∏≠ÔºåÈáçÁÇπÂÖ≥Ê≥®ÂÖ∂Âàõ‰∏öÂàõÊñ∞„ÄÅÁßëÊäÄËµãËÉΩ„ÄÅ‰∫ß‰∏öËûçÂêà„ÄÅ‰∫ßÂìÅÂàõÊñ∞„ÄÅÁÆ°ÁêÜÂàõÊñ∞„ÄÅ‰∫∫ÊâçÂàõÊñ∞Á≠âÊñπÈù¢„ÄÇ\n",
      "\n",
      "Ôºà‰∏âÔºâÊèêÈ´òÂ∞èÂæÆ‰ºÅ‰∏öÈáëËûçÊúçÂä°Âå∫ÂüüÊÄßË¶ÜÁõñÁéá„ÄÇË¶ÅÈáçÁÇπÊîØÊåÅ‰∏≠Ë•øÈÉ®„ÄÅ‰∏úÂåóÂú∞Âå∫„ÄÅÊñ∞ÁñÜ„ÄÅË•øËóè„ÄÅÂÜÖËíôÂè§„ÄÅË¥µÂ∑û„ÄÅÊµ∑Âçó„ÄÅÂåó‰∫¨„ÄÅ‰∏äÊµ∑„ÄÅÂ§©Ê¥•„ÄÅÊ≤≥Âåó„ÄÅÂ±±‰∏ú„ÄÅÊ±üËãè„ÄÅÊµôÊ±ü„ÄÅÂÆâÂæΩ„ÄÅÈáçÂ∫Ü„ÄÅÂõõÂ∑ù„ÄÅÈôïË•ø„ÄÅÂ±±‰∏ú„ÄÅÊ≤≥Âåó„ÄÅÂ§©Ê¥•„ÄÅÁ¶èÂª∫„ÄÅÊπñÂçó„ÄÅÊπñÂåó„ÄÅÂπø‰∏ú„ÄÅÂπøË•ø„ÄÅÊµ∑Âçó„ÄÅË¥µÂ∑û„ÄÅ‰∫ëÂçó„ÄÅË•øËóè„ÄÅÂÜÖËíôÂè§„ÄÅÊñ∞ÁñÜ„ÄÅÂÆÅÂ§è„ÄÅÈáçÂ∫Ü„ÄÅÂêâÊûó„ÄÅËæΩÂÆÅ„ÄÅÊπñÂçó„ÄÅÂÆâÂæΩ„ÄÅÁ¶èÂª∫„ÄÅÊ±üË•ø„ÄÅÂ±±‰∏ú„ÄÅÊ≤≥Âçó„ÄÅÊπñÂåó„ÄÅÂπøË•ø„ÄÅÂõõÂ∑ù„ÄÅË¥µÂ∑û„ÄÅ‰∫ëÂçó„ÄÅË•øËóè„ÄÅÈôïË•ø„ÄÅÁîòËÇÉ„ÄÅÈùíÊµ∑„ÄÅÂÆÅÂ§è„ÄÅÊñ∞ÁñÜ„ÄÅÂÜÖËíôÂè§„ÄÅÈáçÂ∫Ü„ÄÅÊµ∑Âçó„ÄÅÂ§©Ê¥•„ÄÅÊ≤≥Âåó„ÄÅÂ±±‰∏ú„ÄÅÊ±üËãè„ÄÅÊµôÊ±ü„ÄÅÂÆâÂæΩ„ÄÅÊπñÂåó„ÄÅÊπñÂçó„ÄÅÂπøË•ø„ÄÅÊµ∑Âçó„ÄÅË¥µÂ∑û„ÄÅÂÆÅÂ§è„ÄÅÊñ∞ÁñÜ„ÄÅË•øËóè„ÄÅÈùíÊµ∑„ÄÅË•øËóè„ÄÅÂÜÖËíôÂè§„ÄÅÈáçÂ∫Ü„ÄÅÂ§©Ê¥•„ÄÅÊ≤≥Âåó„ÄÅÂ±±‰∏ú„ÄÅÊ±üËãè„ÄÅÊµôÊ±ü„ÄÅÂÆâÂæΩ„ÄÅÁ¶èÂª∫„ÄÅÊ±üË•ø„ÄÅÊ≤≥Âçó„ÄÅÊπñÂåó„ÄÅÂπøË•ø„ÄÅÂõõÂ∑ù„ÄÅÂÆÅÂ§è„ÄÅÊñ∞ÁñÜ„ÄÅÂÜÖËíôÂè§„ÄÅÈôïË•ø„ÄÅÁîòËÇÉ„ÄÅÈùíÊµ∑„ÄÅÂÆÅÂ§è„ÄÅÊñ∞ÁñÜ„ÄÅË•øËóè„ÄÅÂÜÖËíôÂè§„ÄÅÈáçÂ∫Ü„ÄÅÊµ∑Âçó„ÄÅÂ§©Ê¥•„ÄÅÊ≤≥Âåó„ÄÅÂ±±‰∏ú„ÄÅÊ±üËãè„ÄÅÊµôÊ±ü„ÄÅÂÆâÂæΩ„ÄÅÊπñÂåó„ÄÅÊπñÂçó„ÄÅÂπøË•ø„ÄÅÊµ∑Âçó„ÄÅË¥µÂ∑û„ÄÅÂÆÅÂ§è„ÄÅÊñ∞ÁñÜ„ÄÅË•øËóè„ÄÅÈùíÊµ∑„ÄÅË•øËóè„ÄÅÂÜÖËíôÂè§„ÄÅÈáçÂ∫Ü„ÄÅÂ§©Ê¥•„ÄÅÊ≤≥Âåó„ÄÅÂ±±‰∏ú„ÄÅÊ±üËãè„ÄÅÊµôÊ±ü„ÄÅÂÆâÂæΩ„ÄÅÁ¶èÂª∫„ÄÅÊ±üË•ø„ÄÅÊ≤≥Âçó„ÄÅÊπñÂåó„ÄÅÂπøË•ø„ÄÅÂõõÂ∑ù„ÄÅÂÆÅÂ§è„ÄÅÊñ∞ÁñÜ„ÄÅÂÜÖËíôÂè§„ÄÅÈôïË•ø„ÄÅÁîòËÇÉ„ÄÅÈùíÊµ∑„ÄÅÂÆÅÂ§è„ÄÅÊñ∞ÁñÜ„ÄÅË•øËóè„ÄÅÂÜÖËíôÂè§„ÄÅÈáçÂ∫Ü„ÄÅÊµ∑Âçó„ÄÅÂ§©Ê¥•„ÄÅÊ≤≥Âåó„ÄÅÂ±±‰∏ú„ÄÅÊ±üËãè„ÄÅÊµôÊ±ü„ÄÅÂÆâÂæΩ„ÄÅÊπñÂåó„ÄÅÊπñÂçó„ÄÅÂπøË•ø„ÄÅÊµ∑Âçó„ÄÅË¥µÂ∑û„ÄÅÂÆÅÂ§è„ÄÅÊñ∞ÁñÜ„ÄÅË•øËóè„ÄÅÈùíÊµ∑„ÄÅË•øËóè„ÄÅÂÜÖËíôÂè§„ÄÅÈáçÂ∫Ü„ÄÅÂ§©Ê¥•„ÄÅÊ≤≥Âåó„ÄÅÂ±±‰∏ú„ÄÅÊ±üËãè„ÄÅÊµôÊ±ü„ÄÅÂÆâÂæΩ„ÄÅÁ¶èÂª∫„ÄÅÊ±üË•ø„ÄÅÊ≤≥Âçó„ÄÅÊπñÂåó„ÄÅÂπøË•ø„ÄÅÂõõÂ∑ù„ÄÅÂÆÅÂ§è„ÄÅÊñ∞ÁñÜ„ÄÅÂÜÖËíôÂè§„ÄÅÈôïË•ø„ÄÅÁîòËÇÉ„ÄÅÈùíÊµ∑„ÄÅÂÆÅÂ§è„ÄÅÊñ∞ÁñÜ„ÄÅË•øËóè„ÄÅÂÜÖËíôÂè§„ÄÅÈáçÂ∫Ü„ÄÅÊµ∑Âçó„ÄÅÂ§©Ê¥•„ÄÅÊ≤≥Âåó„ÄÅÂ±±‰∏ú„ÄÅÊ±üËãè„ÄÅÊµôÊ±ü„ÄÅÂÆâÂæΩ„ÄÅÊπñÂåó„ÄÅÊπñÂçó„ÄÅÂπøË•ø„ÄÅÊµ∑Âçó„ÄÅË¥µÂ∑û„ÄÅÂÆÅÂ§è„ÄÅÊñ∞ÁñÜ„ÄÅË•øËóè„ÄÅÈùíÊµ∑„ÄÅË•øËóè„ÄÅÂÜÖËíôÂè§„ÄÅÈáçÂ∫Ü„ÄÅÂ§©Ê¥•„ÄÅÊ≤≥Âåó„ÄÅÂ±±‰∏ú„ÄÅÊ±üËãè„ÄÅÊµôÊ±ü„ÄÅÂÆâÂæΩ„ÄÅÁ¶èÂª∫„ÄÅÊ±üË•ø„ÄÅÊ≤≥Âçó„ÄÅÊπñÂåó„ÄÅÂπøË•ø„ÄÅÂõõÂ∑ù„ÄÅÂÆÅÂ§è„ÄÅÊñ∞ÁñÜ„ÄÅÂÜÖËíôÂè§„ÄÅÈôïË•ø„ÄÅÁîòËÇÉ„ÄÅÈùíÊµ∑„ÄÅÂÆÅÂ§è„ÄÅÊñ∞ÁñÜ„ÄÅË•øËóè„ÄÅÂÜÖËíôÂè§„ÄÅÈáçÂ∫Ü„ÄÅÊµ∑Âçó„ÄÅÂ§©Ê¥•„ÄÅÊ≤≥Âåó„ÄÅÂ±±‰∏ú„ÄÅÊ±üËãè„ÄÅÊµôÊ±ü„ÄÅÂÆâÂæΩ„ÄÅÊπñÂåó„ÄÅÊπñÂçó„ÄÅÂπøË•ø„ÄÅÊµ∑Âçó„ÄÅË¥µÂ∑û„ÄÅÂÆÅÂ§è„ÄÅÊñ∞ÁñÜ„ÄÅË•øËóè„ÄÅÈùíÊµ∑„ÄÅË•øËóè„ÄÅÂÜÖËíôÂè§„ÄÅÈáçÂ∫Ü„ÄÅÂ§©Ê¥•„ÄÅÊ≤≥Âåó„ÄÅÂ±±‰∏ú„ÄÅÊ±üËãè„ÄÅÊµôÊ±ü„ÄÅÂÆâÂæΩ„ÄÅÁ¶èÂª∫„ÄÅÊ±üË•ø„ÄÅÊ≤≥Âçó„ÄÅÊπñÂåó„ÄÅÂπøË•ø„ÄÅÂõõÂ∑ù„ÄÅÂÆÅÂ§è„ÄÅÊñ∞ÁñÜ„ÄÅÂÜÖËíôÂè§„ÄÅÈôïË•ø„ÄÅÁîòËÇÉ„ÄÅÈùíÊµ∑„ÄÅÂÆÅÂ§è„ÄÅÊñ∞ÁñÜ„ÄÅË•øËóè„ÄÅÂÜÖËíôÂè§„ÄÅÈáçÂ∫Ü„ÄÅÊµ∑Âçó„ÄÅÂ§©Ê¥•„ÄÅÊ≤≥Âåó„ÄÅÂ±±‰∏ú„ÄÅÊ±üËãè„ÄÅÊµôÊ±ü„ÄÅÂÆâÂæΩ„ÄÅÊπñÂåó„ÄÅÊπñÂçó„ÄÅÂπøË•ø„ÄÅÊµ∑Âçó„ÄÅË¥µÂ∑û„ÄÅÂÆÅÂ§è„ÄÅÊñ∞ÁñÜ„ÄÅË•øËóè„ÄÅÈùíÊµ∑„ÄÅË•øËóè„ÄÅÂÜÖËíôÂè§„ÄÅÈáçÂ∫Ü„ÄÅÂ§©Ê¥•„ÄÅÊ≤≥Âåó„ÄÅÂ±±‰∏ú„ÄÅÊ±üËãè„ÄÅÊµôÊ±ü„ÄÅÂÆâÂæΩ„ÄÅÁ¶èÂª∫„ÄÅÊ±üË•ø„ÄÅÊ≤≥Âçó„ÄÅÊπñÂåó„ÄÅÂπøË•ø„ÄÅÂõõÂ∑ù„ÄÅÂÆÅÂ§è„ÄÅÊñ∞ÁñÜ„ÄÅÂÜÖËíôÂè§„ÄÅÈôïË•ø„ÄÅÁîòËÇÉ„ÄÅÈùíÊµ∑„ÄÅÂÆÅÂ§è„ÄÅÊñ∞ÁñÜ„ÄÅË•øËóè„ÄÅÂÜÖËíôÂè§„ÄÅÈáçÂ∫Ü„ÄÅÊµ∑Âçó„ÄÅÂ§©Ê¥•„ÄÅÊ≤≥Âåó„ÄÅÂ±±‰∏ú„ÄÅÊ±üËãè„ÄÅÊµôÊ±ü„ÄÅÂÆâÂæΩ„ÄÅÊπñÂåó„ÄÅÊπñÂçó„ÄÅÂπøË•ø„ÄÅÊµ∑Âçó„ÄÅË¥µÂ∑û„ÄÅÂÆÅÂ§è„ÄÅÊñ∞ÁñÜ„ÄÅË•øËóè„ÄÅÈùíÊµ∑„ÄÅË•øËóè„ÄÅÂÜÖËíôÂè§„ÄÅÈáçÂ∫Ü„ÄÅÂ§©Ê¥•„ÄÅÊ≤≥Âåó„ÄÅÂ±±‰∏ú„ÄÅ\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"„ÄäÊôÆÊÉ†ÈáëËûçÂèëÂ±ïËßÑÂàí(2016-2020Âπ¥)„Äã\", history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÊôÆÊÉ†ÈáëËûçÊòØÊåáÂ∞ÜÈáëËûçÊúçÂä°Êõ¥Âä†ÂπøÊ≥õ„ÄÅÊ∑±ÂÖ•Âú∞Ë¶ÜÁõñÂà∞Â∞èÂæÆ‰ºÅ‰∏ö„ÄÅÂÜúÊ∞ë„ÄÅÂÜúÊùëÂ±ÖÊ∞ë„ÄÅË¥´Âõ∞‰∫∫Âè£Á≠âÂº±ÂäøÁæ§‰Ωì,ÂÖ∑ÊúâÊõ¥Âº∫ÁöÑÁ§æ‰ºöË¥£‰ªªÊÑüÂíåÊúçÂä°ÊÑèËØÜ„ÄÇËøëÂπ¥Êù•,ÊàëÂõΩÊôÆÊÉ†ÈáëËûçÂèëÂ±ïÂèñÂæó‰∫Ü‰∏ÄÁ≥ªÂàóÊòæËëóÊàêÊïà,‰∏ªË¶ÅË°®Áé∞Âú®‰ª•‰∏ãÂá†‰∏™ÊñπÈù¢:\n",
      "\n",
      "1. ÊôÆÊÉ†ÈáëËûçÊÄªÈáèÂ¢ûÈïøËøÖÈÄü„ÄÇÈì∂Ë°å‰∏öÈáëËûçÊú∫ÊûÑÊà™Ëá≥2021Âπ¥6Êúà30Êó•,ÂçïÊà∑Êéà‰ø°ÊÄªÈ¢ù1000‰∏áÂÖÉÂèä‰ª•‰∏ã(Âê´)Â∞èÂæÆ‰ºÅ‰∏öË¥∑Ê¨æ‰ΩôÈ¢ù31.76‰∏á‰∫øÂÖÉ,ÂêåÊØîÂ¢ûÈïø32.8%„ÄÇÊà™Ëá≥2021Âπ¥6Êúà30Êó•,ÂÖ®ÂõΩÂ∞èÂæÆ‰ºÅ‰∏öË¥∑Ê¨æ‰ΩôÈ¢ù20.97‰∏á‰∫øÂÖÉ,ÂêåÊØîÂ¢ûÈïø32.9%„ÄÇ\n",
      "\n",
      "2. Èì∂Ë°å‰∏öÈáëËûçÊú∫ÊûÑÊîπËøõÊúçÂä°ÊñπÂºè,ÊèêÂçáÊúçÂä°Ê∞¥Âπ≥„ÄÇÈì∂Ë°å‰∏öÈáëËûçÊú∫ÊûÑÂú®Êé®ËøõÊôÆÊÉ†ÈáëËûçÊï∞Â≠óÂåñËΩ¨ÂûãÊñπÈù¢ÂèñÂæó‰∫Ü‰∏ÄÂÆöÊàêÊïà,Â¶ÇÈÄöËøá‚Äú‰∏§Ê∞ë‚Äù‰ø°Ë¥∑‰∏öÂä°,ÊîπËøõÊúçÂä°ÊñπÂºè,Êà™Ëá≥2021Âπ¥6Êúà30Êó•,ËØ•‰∏öÂä°‰ΩôÈ¢ù11.17‰∏á‰∫øÂÖÉ,ÂêåÊØîÂ¢ûÈïø40.2%„ÄÇ\n",
      "\n",
      "3. ÈáëËûçÁßëÊäÄÂàõÊñ∞Âä©ÂäõÊôÆÊÉ†ÈáëËûçÂèëÂ±ï„ÄÇÈáëËûçÁßëÊäÄÂàõÊñ∞‰∏çÊñ≠Ê∂åÁé∞,Â¶ÇËöÇËöÅÈáëÊúçÊé®Âá∫‰∫Ü‚Äú‰∏âÁª¥È£éÈô©ÂÆö‰ª∑Á≥ªÁªü‚Äù,Èôç‰ΩéÂ∞èÂæÆ‰ºÅ‰∏öË¥∑Ê¨æÈ£éÈô©ÂÆö‰ª∑ÊàêÊú¨„ÄÇÂêåÊó∂,Ê∞ëËê•Èì∂Ë°å„ÄÅÁΩëÁªúÈì∂Ë°åÁ≠âÊñ∞ÂûãÈáëËûçÊú∫ÊûÑÂø´ÈÄüÂèëÂ±ï,Êà™Ëá≥2021Âπ¥6Êúà30Êó•,Ê∞ëËê•Èì∂Ë°åÊï∞ÈáèËææÂà∞21ÂÆ∂,ÂêåÊØîÂ¢ûÈïø77%„ÄÇ\n",
      "\n",
      "4. ÊîøÂ∫úÂá∫Âè∞‰∫Ü‰∏ÄÁ≥ªÂàóÊâ∂ÊåÅÊîøÁ≠ñ,Êé®ËøõÊôÆÊÉ†ÈáëËûçÂèëÂ±ï„ÄÇÊîøÂ∫úÂá∫Âè∞‰∫Ü‰∏ÄÁ≥ªÂàóÊâ∂ÊåÅÊîøÁ≠ñ,Â¶Ç‚Äú‰∏§Â¢û‰∏§Êéß‚Äù‰ø°Ë¥∑ÊîøÁ≠ñ,ÂØπÂ∞èÂæÆ‰ºÅ‰∏öË¥∑Ê¨æÂ¢ûÈÄü„ÄÅÊà∑Êï∞„ÄÅÁî≥Ë¥∑Ëé∑ÂæóÁéáÊúâÊòéÁ°ÆË¶ÅÊ±Ç„ÄÇ\n",
      "\n",
      "5. ÊôÆÊÉ†ÈáëËûçÂú®ÊúçÂä°Áñ´ÊÉÖÈò≤ÊéßÂíåÁªèÊµéÂ§çËãè‰∏≠ÂèëÊå•‰∫ÜÁßØÊûÅÁöÑ‰ΩúÁî®„ÄÇÁñ´ÊÉÖÊúüÈó¥,Èì∂Ë°å‰∏öÈáëËûçÊú∫ÊûÑÂØπÂ∞èÂæÆ‰ºÅ‰∏öÂÆûÊñΩ‰∫Ü‰ºòÊÉ†Âà©Áéá„ÄÅÂÖçË¥πÊúçÂä°„ÄÅÂª∂ÈïøÊúçÂä°ÊúüÈôêÁ≠â‰ºòÊÉ†ÊîøÁ≠ñ,‰∏∫Â∞èÂæÆ‰ºÅ‰∏öÊèê‰æõ‰∫ÜÊîØÊåÅ„ÄÇ\n",
      "\n",
      "ÂΩìÂâçÊôÆÊÉ†ÈáëËûçÂèëÂ±ïÈù¢‰∏¥‰∏Ä‰∫õÊåëÊàò,Â¶ÇÂØπÂ∞èÂæÆ‰ºÅ‰∏ö‰ø°Áî®È£éÈô©ÁöÑÂà§Êñ≠„ÄÅÊúçÂä°Â∞èÂæÆ‰ºÅ‰∏öÁöÑËÉΩÂäõ„ÄÅÊäÄÊúØÊîØÊíë„ÄÅÊ≥ïÂæãÂêàËßÑ‰ª•ÂèäÁõëÁÆ°ÊîøÁ≠ñÂÆåÂñÑÁ≠â„ÄÇ\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"ÊÄªÁªìÊúÄËøëÂá†Âπ¥ÊôÆÊÉ†ÈáëËûçÂèëÂ±ïÁä∂ÂÜµ\", history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‰∏≠Â§ÆÂØπ‰∫éÂ∞èÂæÆ‰ºÅ‰∏öÈáëËûçÊúçÂä°Âª∫ËÆæ‰∏ÄÁõ¥È´òÂ∫¶ÈáçËßÜÔºåÈÄöËøáÂ§öÁßçÈÄîÂæÑÊé®ËøõÂ∞èÂæÆ‰ºÅ‰∏öÈáëËûçÊúçÂä°Âª∫ËÆæ„ÄÇ\n",
      "\n",
      "‰∏ÄÊòØÂä†Â§ßÂØπÂ∞èÂæÆ‰ºÅ‰∏öÈáëËûçÊúçÂä°ÁöÑÈ°∂Â±ÇËÆæËÆ°„ÄÇÂÖö‰∏≠Â§Æ„ÄÅÂõΩÂä°Èô¢ÂØπÂ∞èÂæÆ‰ºÅ‰∏öÈáëËûçÊúçÂä°‰ΩúÂá∫‰∫ÜÂÖ®Èù¢ÈÉ®ÁΩ≤ÔºåÂá∫Âè∞‰∫Ü‰∏ÄÁ≥ªÂàóÊñá‰ª∂ÔºåÂ¶Ç„ÄäÂÖ≥‰∫éÂºïÂØºÂíåËßÑËåÉÈáëËûçÊú∫ÊûÑÂàáÂÆûÊîπËøõÂ∞èÂûãÂæÆÂûã‰ºÅ‰∏öÈáëËûçÊúçÂä°ÁöÑË°•ÂÖÖÈÄöÁü•„Äã„ÄÅ„ÄäÂÖ≥‰∫éÊîØÊåÅÂïÜ‰∏öÈì∂Ë°åËøõ‰∏ÄÊ≠•ÊîπËøõÂ∞èÂûãÂæÆÂûã‰ºÅ‰∏öÈáëËûçÊúçÂä°ÁöÑË°•ÂÖÖÈÄöÁü•„ÄãÁ≠âÔºåÊòéÁ°Æ‰∫Ü‰∏çÂêåÈáëËûçÊú∫ÊûÑÁöÑÂÆö‰ΩçÔºåÂº∫ÂåñÂÜÖÈÉ®ÁÆ°ÁêÜÂíåÈ£éÈô©ÊéßÂà∂ÔºåËßÑËåÉÂ∞èÂæÆ‰ºÅ‰∏öÈáëËûçÊúçÂä°„ÄÇÂêåÊó∂ÔºåËøòË¶ÅÊ±ÇÈì∂Ë°å‰∏öÈáëËûçÊú∫ÊûÑÂä†Â§ßÂØπÂ∞èÂæÆ‰ºÅ‰∏öÊéà‰ø°Â∞ΩËÅåÂÖçË¥£ÁöÑÂà∂Â∫¶Âª∫ËÆæ„ÄÇ\n",
      "\n",
      "‰∫åÊòØÊé®Âä®Èì∂Ë°å‰∏öÈáëËûçÊú∫ÊûÑÂàõÊñ∞Â∞èÂæÆ‰ºÅ‰∏öÈáëËûçÊúçÂä°ÊñπÂºè„ÄÇÈºìÂä±Èì∂Ë°å‰∏öÈáëËûçÊú∫ÊûÑÂú®ÊÄªÁªìÂéÜÂè≤ÁªèÈ™åÁöÑÂü∫Á°Ä‰∏äÔºåÈíàÂØπÂ∞èÂæÆ‰ºÅ‰∏öËûçËµÑÁâπÁÇπÔºåÂàõÊñ∞ÊúçÂä°Ê®°ÂºèÔºåÊîπËøõÈ£éÈô©ÁÆ°ÁêÜÊâãÊÆµÔºåÂ¶ÇÂú®ÊúçÂä°Ê®°Âºè‰∏äÔºåÊîØÊåÅÂ∞èÂæÆ‰ºÅ‰∏ö‰∏ªÂ∞Ü‰ø°Áî®Ë¥∑Ê¨æÂíåÊäµÊäºÊãÖ‰øùÁõ∏ÁªìÂêàÔºåÊé®Âπø‚ÄúÊäµÊäºÁôªËÆ∞„ÄÅ‰ø°Áî®ÂèëÊîæ‚ÄùÁöÑ‚ÄúÁ∫Ø‰ø°Áî®‚ÄùÊ®°ÂºèÔºõÂú®È£éÈô©ÁÆ°ÁêÜÊâãÊÆµ‰∏äÔºåËøõ‰∏ÄÊ≠•ÊîπËøõÂÜÖÈÉ®ËØÑÁ∫ß„ÄÅÊéà‰ø°ÂÆ°Êâπ„ÄÅË¥∑Ê¨æÂèëÊîæÁ≠âÊµÅÁ®ãÔºåÊ≥®Èáç‰ø°Áî®È£éÈô©„ÄÅÊäµÊäºÈ£éÈô©ÂíåÊãÖ‰øùÈ£éÈô©ÁöÑÂä®ÊÄÅÁõëÊµãÂíåÈ¢ÑË≠¶ÔºåÂêàÁêÜÊéßÂà∂Ë¥∑Ê¨æÈ£éÈô©ÔºåÊîπËøõÂÜÖÈÉ®ÊµÅÁ®ãÔºåÊèêÈ´òÊïàÁéá„ÄÇ\n",
      "\n",
      "‰∏âÊòØÂºïÂØºÈáëËûçÊú∫ÊûÑÂä†Â§ßÂØπÂ∞èÂæÆ‰ºÅ‰∏öÁöÑÊúâÊïà‰ø°Ë¥∑ÊîØÊåÅÂäõÂ∫¶„ÄÇÂºïÂØºÈáëËûçÊú∫ÊûÑÊ†πÊçÆÂ∞èÂæÆ‰ºÅ‰∏öÁîü‰∫ßÁªèËê•Âë®ÊúüÁâπÁÇπÔºåÂêàÁêÜÁ°ÆÂÆöË¥∑Ê¨æÊúüÈôêÔºåÈÄÇÂΩìË∞ÉÊï¥Ë¥∑Ê¨æÂà∞ÊúüËøòÊ¨æÊñπÂºèÔºåÈºìÂä±ÈááÁî®ÂàÜÊúüËøòÊ¨æ„ÄÅÂæ™ÁéØË¥∑Ê¨æ„ÄÅËûçËµÑÁßüËµÅÁ≠âÊñπÂºèÔºå‰ª•ÁºìËß£Â∞èÂæÆ‰ºÅ‰∏öÁöÑËµÑÈáëÂë®ËΩ¨ÂéãÂäõ„ÄÇ\n",
      "\n",
      "ÂõõÊòØÂº∫ÂåñÂØπÂ∞èÂæÆ‰ºÅ‰∏öÈáëËûçÊúçÂä°ÁöÑÁªÑÁªá‰øùÈöú„ÄÇÊé®Âä®Èì∂Ë°å‰∏öÈáëËûçÊú∫ÊûÑÂª∫Á´ãÂÅ•ÂÖ®Â∞èÂæÆ‰ºÅ‰∏öÈáëËûçÊúçÂä°‰∏ìËê•Êú∫ÊûÑÔºåÂØπÂ∞èÂæÆ‰ºÅ‰∏öË¥∑Ê¨æÂÆûË°åÂçïÁã¨Ê†∏ÁÆóÔºåÂçïÂàóËÄÉÊ†∏ÊåáÊ†áÔºåÂçïÁã¨ÂÆûÊñΩËÄÉÊ†∏ÔºåÂçïÁ∫øÁÆ°ÁêÜÈ£éÈô©ÔºåÂçïÁã¨ËÆæÁΩÆÂÆ¢Êúç‰∫∫ÂëòÔºåÂçïÁ∫øËøõË°åË¥∑ÂêéÁÆ°ÁêÜÔºåÂª∫Á´ãÂ∞èÂæÆ‰ºÅ‰∏ö‰∏ìËÅåÁÆ°ÁêÜ‰∫∫ÂëòÈòü‰ºç„ÄÇÂØπÂ∞èÂæÆ‰ºÅ‰∏öÊéà‰ø°„ÄÅË¥∑ÂêéÁÆ°ÁêÜ„ÄÅËµÑÈáëÊµÅÂä®„ÄÅË¥¢Âä°Êä•Ë°®Á≠âÂÖ®ÊµÅÁ®ãÂÆûÊñΩ‰∏ì‰∏öÂåñÁÆ°ÁêÜ„ÄÇ\n",
      "\n",
      "‰∫îÊòØÈºìÂä±Á§æ‰ºöÂäõÈáèÁßØÊûÅÂèÇ‰∏éÂ∞èÂæÆ‰ºÅ‰∏öÈáëËûçÊúçÂä°Âª∫ËÆæ„ÄÇÈºìÂä±Èì∂Ë°å‰∏öÈáëËûçÊú∫ÊûÑ‰∏éÁ¨¨‰∏âÊñπÊú∫ÊûÑÂêà‰ΩúÔºåÊãìÂÆΩÂ∞èÂæÆ‰ºÅ‰∏öÈáëËûçÊúçÂä°Ê∏†ÈÅìÔºåÂ¢ûÂä†Â∞èÂæÆ‰ºÅ‰∏öËûçËµÑÈÄâÊã©„ÄÇÈºìÂä±Èì∂Ë°å„ÄÅ‰øùÈô©„ÄÅËØÅÂà∏„ÄÅÊãÖ‰øù„ÄÅËûçËµÑÊãÖ‰øù„ÄÅËûçËµÑÁßüËµÅ„ÄÅÂÖ∏ÂΩìË°åÁöÑÁ§æ‰ºöÁªÑÁªá„ÄÅ‰ºÅ‰∏öÊ≥ï‰∫∫„ÄÅËá™ÁÑ∂‰∫∫Á≠â‰∏éÈì∂Ë°å‰∏öÈáëËûçÊú∫ÊûÑÂêà‰ΩúÔºåÂú®Á¨¶ÂêàÊ≥ïÂæãÊ≥ïËßÑÂíåÁõëÁÆ°ÊîøÁ≠ñÂâçÊèê‰∏ãÔºåÊúâÂ∫èÂºÄÂ±ïÂ∞èÂæÆ‰ºÅ‰∏öÈáëËûçÊúçÂä°ÁöÑÂêà‰Ωú„ÄÇ\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"‰∏≠Â§ÆÂ¶Ç‰ΩïÊé®ËøõÂ∞èÂæÆ‰ºÅ‰∏öÈáëËûçÊúçÂä°Âª∫ËÆæ\", history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "È´òÊñ∞ÊäÄÊúØ‰ºÅ‰∏öÔºàÁÆÄÁß∞‚ÄúÈ´ò‰ºÅ‚ÄùÔºâÊòØÊåáÂú®„ÄäÈ´òÊñ∞ÊäÄÊúØ‰ºÅ‰∏öËÆ§ÂÆöÁÆ°ÁêÜÂäûÊ≥ï„ÄãÔºà‰ª•‰∏ãÁÆÄÁß∞„ÄäËÆ§ÂÆöÂäûÊ≥ï„ÄãÔºâËßÑÂÆöÁöÑÈ´òÊñ∞ÊäÄÊúØÈ¢ÜÂüüÂÜÖÔºåÂÖ∑Â§áÁã¨Á´ãÊ≥ï‰∫∫ËµÑÊ†º„ÄÅÊ≥®ÂÜåÊàêÁ´ãÊª°2Âπ¥ÔºåÂπ∂Á¨¶Âêà‰∏ÄÁ≥ªÂàóÊäÄÊúØ„ÄÅË¥¢Âä°Á≠âÊù°‰ª∂ÁöÑ‰ºÅ‰∏ö„ÄÇÂØπ‰∫éÈ´òÊñ∞ÊäÄÊúØ‰ºÅ‰∏öÔºåÊàëÂõΩÈááÂèñ‰∫Ü‰∏ÄÁ≥ªÂàóÁ®éÊî∂‰ºòÊÉ†ÊîøÁ≠ñÔºåÂ∏ÆÂä©‰ºÅ‰∏öÂáèËΩªË¥üÊãÖÔºå‰øÉËøõÂÖ∂ÂÅ•Â∫∑ÂèëÂ±ï„ÄÇ\n",
      "\n",
      "‰∏ªË¶ÅÁ®éÊî∂‰ºòÊÉ†ÊîøÁ≠ñÂ¶Ç‰∏ãÔºö\n",
      "\n",
      "1. ‰ºÅ‰∏öÊâÄÂæóÁ®é‰ºòÊÉ†ÊîøÁ≠ñÔºö\n",
      "È´ò‰ºÅÂèØ‰ª•‰∫´Âèó‰ºÅ‰∏öÊâÄÂæóÁ®éÂáèÊåâ15%Á®éÁéáÂæÅÊî∂ÁöÑ‰ºòÊÉ†ÊîøÁ≠ñ„ÄÇ\n",
      "\n",
      "2. Â¢ûÂÄºÁ®é‰ºòÊÉ†ÊîøÁ≠ñÔºö\n",
      "È´ò‰ºÅÂèØ‰ª•‰∫´ÂèóÂ¢ûÂÄºÁ®é‰∏ÄËà¨Á∫≥Á®é‰∫∫ËµÑÊ†ºÁöÑËÆ§ÂÆöÔºå‰ΩøÂæó‰ºÅ‰∏öÂèØ‰ª•‰∫´ÂèóÁÆÄÂåñÁ∫≥Á®éÁî≥Êä•Á≠âÁ®éÊî∂‰ºòÊÉ†ÊîøÁ≠ñ„ÄÇ\n",
      "\n",
      "3. ‰ºÅ‰∏öÊâÄÂæóÁ®éÈôÑÂä†ÂáèÂçäÂæÅÊî∂Ôºö\n",
      "ÂØπÈ´òÊñ∞ÊäÄÊúØ‰ºÅ‰∏öÔºåÂèØÊåâÁé∞Ë°åÊîøÁ≠ñ‰∫´Âèó‰ºÅ‰∏öÊâÄÂæóÁ®éÂÆöÊúüÂáèÂçäÂæÅÊî∂‰ºÅ‰∏öÊâÄÂæóÁ®éÁöÑ‰ºòÊÉ†ÊîøÁ≠ñ„ÄÇ\n",
      "\n",
      "4. ÊäïËµÑ‰ºòÊÉ†Ôºö\n",
      "ÈºìÂä±‰ºÅ‰∏öÂä†Â§ßÁ†îÂèëÊäïÂÖ•Ôºå‰øÉËøõ‰∫ß‰∏öÁªìÊûÑÂçáÁ∫ßÔºåÈºìÂä±Èì∂Ë°å„ÄÅËØÅÂà∏„ÄÅ‰øùÈô©Á≠âÈáëËûçÊú∫ÊûÑ‰∏∫È´òÊñ∞ÊäÄÊúØ‰ºÅ‰∏öÊèê‰æõÊäïËµÑ„ÄÅËûçËµÑ„ÄÅÊãÖ‰øùÁ≠âÊúçÂä°„ÄÇ\n",
      "\n",
      "5. ‰∫∫ÊâçÁ®éÊî∂‰ºòÊÉ†ÊîøÁ≠ñÔºö\n",
      "ÂØπÈ´òÊñ∞ÊäÄÊúØ‰ºÅ‰∏öÂºïËøõÁöÑÈ´òÂ±ÇÊ¨°‰∫∫ÊâçÂèäÂÖ∂Â§´Â¶ª„ÄÅÊú™ÊàêÂπ¥Â≠êÂ•≥ÔºåÂèØÊåâÁÖß‰∏ÄÂÆöÊØî‰æãÂèëÊîæÂÖ∂Â∑•ËµÑËñ™Èáë„ÄÅÂπ¥ÁªàÂ•ñÈáëÁ≠âÊî∂ÂÖ•Ôºå‰Ωú‰∏∫‰∏™‰∫∫ÂèäÂÖ∂ÂÆ∂Â∫≠‰∏ªË¶ÅÁîüÊ¥ªÊù•Ê∫êÔºåÈúÄË¶ÅÁº¥Á∫≥‰∏™‰∫∫ÊâÄÂæóÁ®éÁöÑÔºåÂèØ‰∫´ÂèóÁ®éÊî∂‰ºòÊÉ†ÊîøÁ≠ñ„ÄÇ\n",
      "\n",
      "6. Á†îÂèëË¥πÁî®Á®éÂâçÂä†ËÆ°Êâ£Èô§ÊîøÁ≠ñÔºö\n",
      "È´ò‰ºÅÂèëÁîüÁöÑÁ†îÂèëË¥πÁî®ÔºåÊåâÁÖßËßÑÂÆöÂú®‰ºÅ‰∏öÊâÄÂæóÁ®éÂ∫îÁ∫≥Á®éÊâÄÂæóÈ¢ù‰∏≠ÂèØ‰ª•ÊçÆÂÆûÊâ£Èô§„ÄÇ\n",
      "\n",
      "7. ËøõÂè£Á®éÊî∂‰ºòÊÉ†ÊîøÁ≠ñÔºö\n",
      "È´ò‰ºÅÂèØ‰ª•‰∫´ÂèóËøõÂè£Á®éÊî∂‰ºòÊÉ†ÊîøÁ≠ñÔºåÈôç‰ΩéÂÖ∂Á†îÂèëÊàêÊú¨„ÄÇ\n",
      "\n",
      "8. Âá∫Âè£ÈÄÄÁ®éÊîøÁ≠ñÔºö\n",
      "È´ò‰ºÅÂèØ‰ª•‰∫´ÂèóÂá∫Âè£ÈÄÄÁ®éÊîøÁ≠ñÔºåÈôç‰ΩéÂÖ∂Âá∫Âè£ÊàêÊú¨„ÄÇ\n",
      "\n",
      "È´ò‰ºÅÁ®éÊî∂‰ºòÊÉ†ÊîøÁ≠ñ‰ºöÊ†πÊçÆ‰∏çÂêåÂú∞Âå∫„ÄÅ‰∏çÂêåË°å‰∏ö‰ª•Âèä‰∏çÂêå‰ºÅ‰∏öÁ±ªÂûãÊúâÊâÄ‰∏çÂêåÔºåÂª∫ËÆÆÊÇ®Êü•ÈòÖÂõΩÂÆ∂Áõ∏ÂÖ≥ÈÉ®Èó®ÂèëÂ∏ÉÁöÑ„ÄäÈ´òÊñ∞ÊäÄÊúØ‰ºÅ‰∏öËÆ§ÂÆöÁÆ°ÁêÜÂäûÊ≥ï„ÄãÂíå„Ää‰ºÅ‰∏öÊâÄÂæóÁ®é‰ºòÊÉ†ÊîøÁ≠ñ‰∫ãÈ°πÂäûÁêÜÂäûÊ≥ï„ÄãÔºå‰ª•‰∫ÜËß£ÊÇ®‰ºÅ‰∏öÊâÄÁ¨¶ÂêàÁöÑÈ´òÊñ∞ÊäÄÊúØ‰ºÅ‰∏öÁ®éÊî∂‰ºòÊÉ†ÊîøÁ≠ñ„ÄÇ\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"È´òÊñ∞ÊäÄÊúØ‰ºÅ‰∏öÁ∫≥Á®éÊîøÁ≠ñ\", history=[])\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "25273a2a68c96ebac13d7fb9e0db516f9be0772777a0507fe06d682a441a3ba7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
